{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ce12ab",
   "metadata": {},
   "source": [
    "# Mô hình phân loại email/sms rác (spam classification) sử dụng machine learning. Quy trình theo chuẩn NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0a669",
   "metadata": {},
   "source": [
    "# CAC QUY TRÌNH\n",
    "## 1. Tiền xử lý dữ liệu: Đọc dữ liệu CSV, làm sạch văn bản (bỏ URL, email, ký tự thừa, chuyển lowercase), cắt từ (tokenize), loại bỏ từ vô nghĩa (stopwords), và chuẩn hóa từ (lemmatization). Mục đích: làm sạch dữ liệu, giúp mô hình học hiểu quả hơn (tăng accuracy 10-20%)\n",
    "\n",
    "## 2. Vector hóa (Feature Extraction): Chuyển văn bản thành số (sử dụng TF-IDF) để mô hình có thể xử lú. Mục đích: Văn bản không phải số, nên cần biểu diễn dưới dạng vector để tính toán\n",
    "\n",
    "## 3. Chuẩn bị nhãn và huấn luyện mô hình: Map nhãn \"ham/spam\" (ham là không vi phạm/ spam là vi phạm) thành 0/1, chia train/test, huấn luyện Neive Bayes (hoặc Logistic Regresion). Mục đích: Học từ dữ liệu để dự đoán \"spam/ham\", đánh giá accuracy/recall.\n",
    "\n",
    "## 4. Dự đoán và đánh giá: Dự đoán trên dữ liệu mới. Mục đích: Kiểm tra mô hình thực tế.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c540ae",
   "metadata": {},
   "source": [
    "# Thực hành"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e62406",
   "metadata": {},
   "source": [
    "## 1. Đọc dữ liệu từ file spam.csv vào DataFrame để chuẩn bị xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c04f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_model=pd.read_csv(\"dataset/spam.csv\")\n",
    "print(train_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef93fd8",
   "metadata": {},
   "source": [
    "## 2.Chuyển hóa dữ liệu chữ hoa về thành chữ thường. (Khi phân loại 'Hello' và 'hello' được coi là giống nhau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "939e3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model[\"Text\"]=train_model[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25bc5a5",
   "metadata": {},
   "source": [
    "### 3. Xây dựng hàm để làm sạch dữ liệu Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a20c5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):  # Xử lý null\n",
    "        return \"\"\n",
    "    text = text.lower()  # Kết hợp lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # Bỏ URL\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)         # Bỏ email\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)        # Bỏ số, ký tự (giữ a-z và khoảng trắng)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Bỏ punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # Bỏ khoảng trắng thừa\n",
    "    return text\n",
    "\n",
    "train_model['text_clean'] = train_model[\"Text\"].apply(clean_text) # thêm mới cột clean này.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e4614",
   "metadata": {},
   "source": [
    "### Mục đích: văn bản chứa nhiều thông tin thừa -> làm sạch để giữ lại phần từ ngữ có nghĩa. Việc này giúp giảm nhiễu, tăng hiệu quả mô hình.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69e200c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Label', 'Text', 'text_clean'], dtype='object')\n",
      "Trước clean:\n",
      " 0    go until jurong point, crazy.. available only ...\n",
      "1                        ok lar... joking wif u oni...\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3    u dun say so early hor... u c already then say...\n",
      "4    nah i don't think he goes to usf, he lives aro...\n",
      "Name: Text, dtype: object\n",
      "Sau clean:\n",
      " 0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in a wkly comp to win fa cup final ...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i don t think he goes to usf he lives arou...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_model.columns)\n",
    "print(\"Trước clean:\\n\", train_model['Text'].head())\n",
    "print(\"Sau clean:\\n\", train_model['text_clean'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e17f4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\ADMIN/nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')   # tải tokenizer\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b8ca2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.9.1\n",
      "NLTK data paths: ['C:\\\\Users\\\\ADMIN/nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n",
      "Đang preprocess tokens...\n",
      "                                          text_clean  \\\n",
      "0  go until jurong point crazy available only in ...   \n",
      "1                            ok lar joking wif u oni   \n",
      "2  free entry in a wkly comp to win fa cup final ...   \n",
      "3        u dun say so early hor u c already then say   \n",
      "4  nah i don t think he goes to usf he lives arou...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [jurong, point, crazy, available, bugis, great...  \n",
      "1                            [lar, joking, wif, oni]  \n",
      "2  [free, entry, wkly, comp, win, cup, final, tkt...  \n",
      "3               [dun, say, early, hor, already, say]  \n",
      "4        [nah, think, go, usf, life, around, though]  \n",
      "Ví dụ tokens dòng 0: ['jurong', 'point', 'crazy', 'available', 'bugis', 'great', 'world', 'buffet', 'cine', 'got']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ssl  # Fix SSL nếu cần\n",
    "\n",
    "# Fix SSL cho Windows/Anaconda (nếu download fail)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download đầy đủ (force nếu cần)\n",
    "nltk.download('punkt', force=False)  # Giữ cũ nếu có\n",
    "nltk.download('punkt_tab')  # Key: Download tokenizer mới\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Cho lemmatizer đầy đủ\n",
    "\n",
    "# Kiểm tra version và path (debug)\n",
    "print(\"NLTK version:\", nltk.__version__)\n",
    "print(\"NLTK data paths:\", nltk.data.path)\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #tạo tập hợp để lọc từ vô nghĩa.\n",
    "lemmatizer = WordNetLemmatizer() # để chuẩn hóa từ\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return [] # Nếu text rỗng thì trả về list rỗng\n",
    "    try:\n",
    "        # Thử word_tokenize với punkt_tab\n",
    "        tokens = word_tokenize(text.lower())  # Lowercase ở đây\n",
    "        filtered_tokens = []\n",
    "        for w in tokens:\n",
    "            if w not in stop_words and len(w) > 2 and w.isalpha():  # Bỏ từ ngắn, không chữ cái\n",
    "                try:\n",
    "                    lemma = lemmatizer.lemmatize(w)\n",
    "                    filtered_tokens.append(lemma)\n",
    "                except Exception:\n",
    "                    filtered_tokens.append(w)\n",
    "        return filtered_tokens\n",
    "    except LookupError as e:  # Nếu vẫn lỗi punkt_tab\n",
    "        print(f\"Tokenizer error: {e}. Fallback to split().\")\n",
    "        # Fallback: split đơn giản (không cần punkt)\n",
    "        tokens = text.lower().split()\n",
    "        filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens \n",
    "                           if w not in stop_words and len(w) > 2 and w.isalpha()]\n",
    "        return filtered_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khác: {e}\")\n",
    "        return []\n",
    "\n",
    "# Áp dụng (sẽ dùng fallback nếu cần, chạy nhanh)\n",
    "print(\"Đang preprocess tokens...\")\n",
    "train_model['tokens'] = train_model['text_clean'].fillna('').apply(preprocess_text)\n",
    "print(train_model[['text_clean', 'tokens']].head())\n",
    "print(\"Ví dụ tokens dòng 0:\", train_model['tokens'].iloc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b2c37",
   "metadata": {},
   "source": [
    "## Thêm cột tokens vào train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01d9eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text  \\\n",
      "0   ham  go until jurong point, crazy.. available only ...   \n",
      "1   ham                      ok lar... joking wif u oni...   \n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
      "3   ham  u dun say so early hor... u c already then say...   \n",
      "4   ham  nah i don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                          text_clean  \\\n",
      "0  go until jurong point crazy available only in ...   \n",
      "1                            ok lar joking wif u oni   \n",
      "2  free entry in a wkly comp to win fa cup final ...   \n",
      "3        u dun say so early hor u c already then say   \n",
      "4  nah i don t think he goes to usf he lives arou...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [jurong, point, crazy, available, bugis, great...  \n",
      "1                            [lar, joking, wif, oni]  \n",
      "2  [free, entry, wkly, comp, win, cup, final, tkt...  \n",
      "3               [dun, say, early, hor, already, say]  \n",
      "4        [nah, think, go, usf, life, around, though]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_model['tokens'] = train_model['tokens'].apply(lambda x: [w for w in x if w not in stop_words])\n",
    "print(train_model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8539e",
   "metadata": {},
   "source": [
    "## TfidfVectorizer không chấp nhận danh sách token trực tiếp; nó cần chuỗi văn bản để tự động tách từ (tokenize). Việc nối token thành chuỗi đảm bảo dữ liệu tương thích với vectorizer. Tư duy: Chuẩn bị dữ liệu đúng định dạng để tránh lỗi và tận dụng tối đa chức năng của thư viện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 6800)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF cần chuỗi, nên nối lại tokens thành 1 string\n",
    "train_model['text_final'] = train_model['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer() # khởi tạo đối tượng\n",
    "X = vectorizer.fit_transform(train_model['text_final']) # học từ đoeẻm từ các văn bản trong text_final\n",
    "# chuyển đổi văn bản thành một vecctor TF-IDF tạo ra ma trận thưa X\n",
    "\n",
    "print(X.shape)  # số dòng x số từ (emails x vocab)\n",
    "# số dòng = số văn bản trong train_model[text_final] (số mail)\n",
    "# số cột = số từ duy nhất trong từ điển (vocab size) do TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ba13c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước ma trận TF-IDF: (5572, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Khởi tạo TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # chọn 5000 đặc trưng quan trọng nhất\n",
    "\n",
    "# Fit và transform train data\n",
    "X = tfidf.fit_transform(train_model['text_clean'])  # text_clean là cột đã xử lý văn bản\n",
    "\n",
    "print(\"Kích thước ma trận TF-IDF:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad5ad44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label  label_num\n",
      "0   ham          0\n",
      "1   ham          0\n",
      "2  spam          1\n",
      "3   ham          0\n",
      "4   ham          0\n"
     ]
    }
   ],
   "source": [
    "# Giả sử cột nhãn gốc tên là 'label'\n",
    "# spam = 1, ham = 0\n",
    "train_model['label_num'] = train_model['Label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(train_model[['Label', 'label_num']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6eccdec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9650224215246637\n",
      "\n",
      "Báo cáo phân loại:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       1.00      0.74      0.85       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.87      0.91      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Y là nhãn spam/ham\n",
    "y = train_model['label_num']  # giả sử bạn đã đổi 'spam' = 1, 'ham' = 0\n",
    "\n",
    "# Chia dữ liệu train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Khởi tạo model Naive Bayes\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Đánh giá\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nBáo cáo phân loại:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83e98790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán: [0]\n"
     ]
    }
   ],
   "source": [
    "sample = [\"Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\"]\n",
    "\n",
    "sample_vec = tfidf.transform(sample)\n",
    "print(\"Dự đoán:\", model.predict(sample_vec))  # 1 = spam, 0 = ham\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
